{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Project Example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from tensorflow import keras\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "\n",
    "import csv\n",
    "import sys\n",
    "sys.path.append('../astroML-coral/Tools')\n",
    "sys.path.append('../astroML-coral/Data')\n",
    "import datascrub as scrub\n",
    "import mldataconfig as dtc\n",
    "from astropy.io import fits\n",
    "import numpy as np\n",
    "import pprint\n",
    "import progressbar\n",
    "import gc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Python: 3.8.8 (tags/v3.8.8:024d805, Feb 19 2021, 13:18:16) [MSC v.1928 64 bit (AMD64)]\n",
      "tensorflow: 2.4.1\n",
      "keras: 2.4.0\n",
      "pandas: 1.2.3\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "print(\"Python: {}\".format(sys.version))\n",
    "print(\"tensorflow: {}\".format(tf.__version__))\n",
    "print(\"keras: {}\".format(keras.__version__))\n",
    "print(\"pandas: {}\".format(pd.__version__))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data Setup\n",
    "\n",
    "It is assumed that the shell scripts for bulk downloads from the MAST archives have been manually converted to .txt by saving as such after removing the shebang line.\n",
    "The contents of the .txt end up being referenced with exoplanet status disposition from the Kepler pipeline, and all that written to a .csv. The contents of that csv are then split into more manageable chunks.\n",
    "\n",
    "Note: data is saved in files rather than used as objects because RAM is expensive\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'list'>\n",
      "Kepler pipeline data converted\n",
      "Successfuly pulled system data from:  Data/kepler_lightcurves_Q03_long.txt\n"
     ]
    }
   ],
   "source": [
    "scrub.exofinder('Data/kepler_lightcurves_Q03_long.txt', 'example.csv', False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['757450', 'https://mast.stsci.edu/api/v0.1/Download/file?uri=mast:Kepler/url/missions/kepler/lightcurves/0007/000757450//kplr000757450-2009350155506_llc.fits', '1']\n"
     ]
    }
   ],
   "source": [
    "split1, split2, full = dtc.datasplit('example.csv', 10000)\n",
    "print(split1[0])\n",
    "dat = list(dtc.chunks(split1, 5000))\n",
    "\n",
    "# Do what is needed with the split data then delete. Python memory management woooo!\n",
    "\n",
    "del split1, split2, full"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Getting Flux Data\n",
    "\n",
    "Mass downloads of flux values from the MAST archive. Everything gets referenced via KeplerID. Again, refer to tools scripts for details\n",
    "\n",
    "Warning: Limit queries to ~5000 objects at a time (hence further chunking in previous cell). This takes up a lot of memory.\n",
    "         In efforts to not DDOS the archive, I included two converted lists in the data.zip"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 99% (4993 of 5000) |################### | Elapsed Time: 0:01:22 ETA:   0:00:00"
     ]
    }
   ],
   "source": [
    "dtc.getfluxes(dat[0], 'example1.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 99% (4998 of 4999) |################### | Elapsed Time: 0:43:14 ETA:   0:00:00"
     ]
    }
   ],
   "source": [
    "dtc.getfluxes(dat[1], 'example2.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here we use pandas to read out the .csv data and save it as numpy arrays"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(4998, 4372)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "traindat = pd.read_csv(\"example1.csv\", error_bad_lines=False)\n",
    "train = traindat.to_numpy()\n",
    "testdat = pd.read_csv(\"example2.csv\", error_bad_lines=False)\n",
    "test = testdat.to_numpy()\n",
    "\n",
    "\n",
    "\n",
    "train.shape\n",
    "test.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And this is how the data is split into pices for training and testing of the ML algo\n",
    "Should probably be a function but some weird bits that should be showm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = train[:,2:]\n",
    "y = train[:,1]\n",
    "test_x = test[:,2:]\n",
    "test_y = test[:,1]\n",
    "x = x[:, ~np.isnan(x).any(axis=0)]\n",
    "x = dtc.normalize(x)\n",
    "\n",
    "test_x =test_x[:, ~np.isnan(test_x).any(axis=0)]\n",
    "test_x = dtc.normalize(test_x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train dataset shape: (4999, 4134)\n",
      "Train-res dataset shape: (4999,)\n",
      "Train dataset shape: (4998, 4134)\n",
      "Train-res dataset shape: (4998,)\n"
     ]
    }
   ],
   "source": [
    "print(f\"Train dataset shape: {x.shape}\")\n",
    "print(f\"Train-res dataset shape: {y.shape}\")\n",
    "print(f\"Train dataset shape: {test_x.shape}\")\n",
    "print(f\"Train-res dataset shape: {test_y.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Machine Learning\n",
    "\n",
    "Here we create a keras model with an input layer, interior layer with 'relu' activation function, and an output layer with 4 members, as this is supposed to identify whether the data represents a confirmed, candidate, false positive, or no planet at all. Again, this could be implemented as external functions but the internal workings are on display for the purposes of this project"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/20\n",
      "157/157 [==============================] - 1s 2ms/step - loss: 0.5071 - accuracy: 0.9080\n",
      "Epoch 2/20\n",
      "157/157 [==============================] - 0s 2ms/step - loss: 0.3735 - accuracy: 0.9504\n",
      "Epoch 3/20\n",
      "157/157 [==============================] - 0s 2ms/step - loss: 0.5476 - accuracy: 0.9479\n",
      "Epoch 4/20\n",
      "157/157 [==============================] - 0s 2ms/step - loss: 0.6094 - accuracy: 0.9522\n",
      "Epoch 5/20\n",
      "157/157 [==============================] - 0s 2ms/step - loss: 0.3507 - accuracy: 0.9515\n",
      "Epoch 6/20\n",
      "157/157 [==============================] - 0s 2ms/step - loss: 0.3107 - accuracy: 0.9485\n",
      "Epoch 7/20\n",
      "157/157 [==============================] - 0s 2ms/step - loss: 0.2880 - accuracy: 0.9511\n",
      "Epoch 8/20\n",
      "157/157 [==============================] - 0s 2ms/step - loss: 0.3273 - accuracy: 0.9509\n",
      "Epoch 9/20\n",
      "157/157 [==============================] - 0s 2ms/step - loss: 0.5347 - accuracy: 0.9510\n",
      "Epoch 10/20\n",
      "157/157 [==============================] - 0s 2ms/step - loss: 0.4548 - accuracy: 0.9546\n",
      "Epoch 11/20\n",
      "157/157 [==============================] - 0s 2ms/step - loss: 0.5150 - accuracy: 0.9520\n",
      "Epoch 12/20\n",
      "157/157 [==============================] - 0s 2ms/step - loss: 0.3100 - accuracy: 0.9520\n",
      "Epoch 13/20\n",
      "157/157 [==============================] - 0s 2ms/step - loss: 0.4381 - accuracy: 0.9505\n",
      "Epoch 14/20\n",
      "157/157 [==============================] - 0s 2ms/step - loss: 0.3148 - accuracy: 0.9532\n",
      "Epoch 15/20\n",
      "157/157 [==============================] - 0s 2ms/step - loss: 0.2974 - accuracy: 0.9534\n",
      "Epoch 16/20\n",
      "157/157 [==============================] - 0s 2ms/step - loss: 0.3254 - accuracy: 0.9542\n",
      "Epoch 17/20\n",
      "157/157 [==============================] - 0s 2ms/step - loss: 0.4953 - accuracy: 0.9524\n",
      "Epoch 18/20\n",
      "157/157 [==============================] - 0s 2ms/step - loss: 0.2906 - accuracy: 0.9571\n",
      "Epoch 19/20\n",
      "157/157 [==============================] - 0s 2ms/step - loss: 0.2879 - accuracy: 0.9490\n",
      "Epoch 20/20\n",
      "157/157 [==============================] - 0s 2ms/step - loss: 0.2905 - accuracy: 0.9517\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x21b88356b50>"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = Sequential()\n",
    "model.add(Dense(64))\n",
    "model.add(Dense(128, activation='relu'))\n",
    "model.add(Dense(4))\n",
    "model.compile(optimizer='adam',\n",
    "              loss=tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True),\n",
    "              metrics=['accuracy'])\n",
    "model.fit(x, y, epochs=20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense (Dense)                (None, 64)                264640    \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 128)               8320      \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, 4)                 516       \n",
      "=================================================================\n",
      "Total params: 273,476\n",
      "Trainable params: 273,476\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.summary()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Analysis\n",
    "\n",
    "While the model provides a quick first-pass analysis of the data, it still skews heavily towards no exoplanets. To rectify this, a large amount of either data processing or statistical analysis will need to be done. The model will need to be trained on a more balanced dataset (roughly equal numbers of no planet, candidate, confirmed, or false positive), or determination of what threshold probabilities indicate something about the system. As for binary detection of presence of an exoplanet, see the Flux_algo notebook."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "157/157 [==============================] - 0s 945us/step - loss: 0.2847 - accuracy: 0.9484\n",
      "\n",
      "Test accuracy: 0.9483793377876282\n",
      "2.0\n",
      "[0.9554491  0.00249979 0.02469294 0.01735814]\n"
     ]
    }
   ],
   "source": [
    "test_loss, test_acc = model.evaluate(test_x,  test_y, verbose=1)\n",
    "\n",
    "\n",
    "print('\\nTest accuracy:', test_acc)\n",
    "probability_model = tf.keras.Sequential([model,\n",
    "                                     tf.keras.layers.Softmax()])\n",
    "predictions = probability_model.predict(test_x)\n",
    "\n",
    "print(test_y[31])\n",
    "print(predictions[31])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Score:  [0.2598845958709717, 0.9531906247138977]\n",
      "Train Score:  [0.28468242287635803, 0.9483793377876282]\n",
      "predictions: [[ 4.589682   -0.61855555  0.9471797   0.92334205]\n",
      " [ 5.47079    -0.8964104   1.8064309   1.266984  ]\n",
      " [ 5.451804   -0.8904179   1.7880336   1.2599893 ]]\n"
     ]
    }
   ],
   "source": [
    "trainScore = model.evaluate(x, y, verbose=0)\n",
    "print(\"Train Score: \", trainScore)\n",
    "testScore = model.evaluate(test_x, test_y, verbose=0)\n",
    "print(\"Train Score: \", testScore)\n",
    "predictions = model.predict(test_x[:3])\n",
    "print(\"predictions:\", predictions)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Creating a TSFLite model\n",
    "\n",
    "Simple generation of a tensorflow lite model that can be run on other hardware. Currently unable to make it run on the Google Coral due to the massive datasets involved."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: C:\\Users\\BARTEK~1\\AppData\\Local\\Temp\\tmpq7y5anw0\\assets\n"
     ]
    }
   ],
   "source": [
    "converter = tf.lite.TFLiteConverter.from_keras_model(model)\n",
    "tflite_model = converter.convert()\n",
    "\n",
    "# Save the model.\n",
    "with open('exoalgo.tflite', 'wb') as f:\n",
    "    f.write(tflite_model)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
